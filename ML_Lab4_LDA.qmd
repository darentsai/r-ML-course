---
title: "Statistical and Machine Learning"
subtitle: "Lab4: Classification <br> Linear Discriminant Analysis (LDA)"
author: "Tsai, Dai-Rong"
format:
  revealjs:
    theme: default
    echo: true
    smaller: true
    scrollable: true
    slide-number: true
    auto-stretch: false
    history: false
    pdf-max-pages-per-slide: 5
    embed-resources: true
tbl-cap-location: bottom
---

## Dataset

```{r echo = FALSE}
options(digits = 5, width = 100)
```

```{css echo = FALSE}
.reveal table, ul ul li{
  font-size: smaller;
}
```

> ***Edgar Anderson's Iris Data***

```{r}
# Set random seed
set.seed(12345)

# Packages
library(MASS) # for lad, qda
```

- Response
    - `Species`: setosa / versicolor / virginica
- Predictors
    - `Sepal.Length`
    - `Sepal.Width`
    - `Petal.Length`
    - `Petal.Width`

---

::: {.panel-tabset}

### Preview

```{r}
dim(iris)
head(iris)
proportions(table(iris$Species))
```

### Data Structure

```{r}
str(iris)
```

:::

## Create Training/Testing Partitions

- Split data into 80% training set and 20% test set

```{r}
nr <- nrow(iris)
train.id <- sample(nr, nr * 0.8)

training <- iris[train.id, ]
testing <- iris[-train.id, ]
```

- Check dimension

```{r}
dim(training)
dim(testing)
```

## Linear Discriminant Analysis (LDA)

```{r}
lda.mod <- lda(Species ~ ., training, prior = c(1,1,1)/3)
```

::: {.callout-note appearance="minimal"}

If `prior` is unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.

:::

::: {.callout-tip}

### Components of an `lda` object

- `prior`: the prior probabilities used.
- `means`: the group means.
- `scaling` : a matrix which transforms observations to discriminant functions, normalized so that within groups covariance matrix is spherical.
- `svd`: the singular values, which give the ratio of the between- and within-group standard deviations on the linear discriminant variables. Their squares are the canonical F-statistics.

:::

```{r echo = FALSE}
options(max.print = 20)
```

```{r}
lda.mod
```

```{r}
#| fig-width: 7

plot(lda.mod, col = as.integer(training$Species)+1, cex = 0.5)
```

---

- Retrieve the data used to fit the `lda` model

```{r}
lda.fit <- predict(lda.mod)
lda.fit
```

:::: {.columns}

::: {.column width="50%"}

```{r}
#| fig-height: 7

ldahist(lda.fit$x[, 1], g = training$Species, type = "both")
title("LD1", cex.main = 2)
```

:::

::: {.column width="50%"}

```{r}
#| fig-height: 7

ldahist(lda.fit$x[, 2], g = training$Species, type = "both")
title("LD2", cex.main = 2)
```

:::

::::

- Computation of Linear Discriminants (`LDs`)

```{r}
(Z <- lda.mod$scaling)
X <- model.matrix(Species ~ ., training)[, -1]
center <- t(lda.mod$prior) %*% lda.mod$means
Xc <- scale(X, center = center, scale = FALSE)
LD <- Xc %*% Z
LD
all.equal(LD, lda.fit$x)
```

---

- Linear Decision boundaries

```{r}
#| fig-width: 7
#| code-fold: true
#| code-summary: "codes for plot"

library(ggplot2)

lda.grid.mod <- lda(class ~ LD1 + LD2, data.frame(lda.fit$x, class = lda.fit$class))
ld1 <- seq(min(lda.fit$x[, 1]), max(lda.fit$x[, 1]), length.out = 200)
ld2 <- seq(min(lda.fit$x[, 2]), max(lda.fit$x[, 2]), length.out = 200)
lda.grid <- expand.grid(LD1 = ld1, LD2 = ld2)
lda.grid.post <- cbind(lda.grid, predict(lda.grid.mod, newdata = lda.grid)$posterior)

ggplot(data.frame(lda.fit$x, class = training$Species), aes(x = LD1, y = LD2)) +
  geom_point(aes(colour = class, shape = class)) +
  geom_contour(data = lda.grid.post, aes(z = versicolor), breaks = 0.5, colour = 1) +
  theme_bw()
```

## Quadratic Discriminant Analysis (QDA)

```{r}
qda.mod <- qda(Species ~ ., training, prior = c(1,1,1)/3)
qda.mod
```

- Quadratic Decision boundaries

```{r}
#| fig-width: 7
#| code-fold: true
#| code-summary: "codes for plot"

library(ggplot2)

qda.fit <- predict(qda.mod)
qda.grid.mod <- qda(class ~ LD1 + LD2, data.frame(lda.fit$x, class = qda.fit$class))
qda.grid.post <- cbind(lda.grid, predict(qda.grid.mod, newdata = lda.grid)$posterior)

ggplot(data.frame(lda.fit$x, class = training$Species), aes(x = LD1, y = LD2)) +
  geom_point(aes(colour = class, shape = class)) +
  geom_contour(data = qda.grid.post, aes(z = versicolor), breaks = 0.5, colour = 1) +
  theme_bw()
```

## Prediction

```{r}
lda.pred <- predict(lda.mod, testing)
qda.pred <- predict(qda.mod, testing)
```

- LDA

```{r}
table(Reality = testing$Species, Prediction = lda.pred$class)
mean(testing$Species == lda.pred$class)
```

- QDA

```{r}
table(Reality = testing$Species, Prediction = qda.pred$class)
mean(testing$Species == qda.pred$class)
```
